#!@cs_python@
#-------------------------------------------------------------------------------
#   This file is part of the Code_Saturne Solver.
#
#   Copyright (C) 2009-2010  EDF
#
#   Code_Saturne is free software; you can redistribute it and/or modify it
#   under the terms of the GNU General Public License as published by the
#   Free Software Foundation; either version 2 of the License,
#   or (at your option) any later version.
#
#   Code_Saturne is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty
#   of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public Licence
#   along with the Code_Saturne Preprocessor; if not, write to the
#   Free Software Foundation, Inc.,
#   51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
#-------------------------------------------------------------------------------

#===============================================================================
# Batch options for different queuing systems
#===============================================================================
#
# Batch options for LSF (example: CCRT's platine)
# =====================
#
#BSUB -n 2
#BSUB -W 00:05
#BSUB -o nameandcaseo.%J
#BSUB -e nameandcasee.%J
#BSUB -J nameandcase
#
#  -n : number of processors
#  -W : walltime as hh:mm
#  -o : output file name
#  -e : error file name
#  -J : job name
#
#-------------------------------------------------------------------------------
#
# Batch options for PBS or Torque (example: Chatou cluster)
# ===============================
#
#PBS -l nodes=4:ppn=2
#PBS -l walltime=1:00:00
#PBS -l mem=320mb
#
#PBS -j eo
#PBS -N nameandcase
#
#  nodes    : number of nodes
#  ppn      : number of process per node
#  walltime : wall clock time (hh:mm:ss)
#  mem      : memory
#
#WARNING: when coupling with SYRTHES, 1 processor will be reserved for each
#         instance of SYRTHES. The Kernel will be executed on the remaining
#         processors, so make sure to reserve a sufficiently high number
#         of processors.
#
#-------------------------------------------------------------------------------
#
# Batch options for Sun Grid Engine (example: University of Manchester MACE)
# =================================
#
# set the name of the job
##$ -N nameandcase
#
# request between 2 and 4 slots
##$ -pe mpich 2-4
#
# Execute the job from the current working directory
# Job output will appear in this directory
##$ -cwd
#   can use -o dirname to redirect stdout
#   can use -e dirname to redirect stderr

#  Export these environment variables
##$ -v MPI_HOME

#set -x
#
#-------------------------------------------------------------------------------
#
# Batch options for IBM LoadLeveler (example: IBM cluster under AIX)
# =================================
#
#@ shell = /bin/sh
#
#@ job_name = nameandcase
#
#@ job_type = parallel
#@ cpus = 128
#@ node_usage = not_shared
#
#@ network.MPI = csss,shared,US
#@ bulkxfer = yes
#
#@ wall_clock_limit = 00:20:00
#@ account_no = z001
#
#@ output = $(job_name).$(schedd_host).$(jobid).out
#@ error  = $(job_name).$(schedd_host).$(jobid).err
#@ notification = never
#
#@ queue
# suggested environment settings:
#  export MP_EAGER_LIMIT=65536
#  export MP_SHARED_MEMORY=yes
#  export MEMORY_AFFINITY=MCM
#  export MP_TASK_AFFINITY=MCM
#
#===============================================================================
# Import required Python modules
#===============================================================================
#
import datetime
import os
import os.path
import sys

# Trick so that one doesn't have to set the PYTHONPATH variable
sys.path.insert(0, '@pkgpythondir@')

import cs_config
import cs_check_consistency

from cs_exec_environment import *
from cs_case_domain import *
from cs_case import *

#===============================================================================
# User variables
#
# Variables set to 'None' will be determined automatically
#===============================================================================

casedir = 'CASEDIRNAME'

MESHES = None
REORIENT = False

JOIN = None
PERIODICITY = None

THERMOCHEMISTRY_DATA = None
METEO_DATA = None
USER_INPUT_FILES = None
USER_OUTPUT_FILES = None

# Number of processors associated with resolution domain (auto if none)

N_PROCS = None
N_PROCS_MIN = 1
N_PROCS_MAX = None

# Partition options (if separate run from resolution)

PARTITION_LIST = None

# Solver options

PARAMETERS = None
SOLCOM = False
CUT_WARPED_FACES = None
CHECK_ARGS = None
OUTPUT_ARGS = None

# Optional list of hosts

HOSTS_LIST = None

# Operations to execute

EXEC_PREPROCESS = True
EXEC_PARTITION = True
EXEC_SOLVER = True

# Adaptation (not operational yet)

HOMARD_PREFIX = None

# Execution prefix (if forced)

CS_TMP_PREFIX = None

#-------------------------------------------------------------------------------

def def_exec_prefix():
    """
    Define execution directory prefix.
    """

    if CS_TMP_PREFIX != None:
        return os.path.join(CS_TMP_PREFIX, tmp_Saturne)

    exec_prefix = None

    # Preferential base working directories

    if sys.platform == 'win32' or sys.platform == 'win64':
        user = os.getenv('USERNAME')
    else:
        user = os.getenv('USER')

        exec_prefix_prefs = [os.getenv('SCRATCHDIR'),
                             os.path.join('/scratch', user),
                             os.path.join('/local00/users', user),
                             os.path.join('/local01/users', user)]

    if os.getenv('TMPDIR') != '/tmp':
        exec_prefix_prefs.append(os.getenv('TMPDIR'))

    exec_prefix_prefs.append(os.getenv('HOME'))

    for prefix in exec_prefix_prefs:
        if prefix != None and os.path.isdir(prefix):
            exec_prefix = os.path.join(prefix, 'tmp_Saturne')
            break

    return exec_prefix

#-------------------------------------------------------------------------------

if __name__ == '__main__':

    # Values in case and associated domain set from global variables

    d = domain(meshes = MESHES,
               reorient = REORIENT,
               join = JOIN,
               periodicity = PERIODICITY,
               partition_list = PARTITION_LIST,
               param = PARAMETERS,
               solcom = SOLCOM,
               cut_warped_faces = CUT_WARPED_FACES,
               mode_args = CHECK_ARGS,
               logging_args = OUTPUT_ARGS,
               thermochemistry_data = THERMOCHEMISTRY_DATA,
               meteo_data = METEO_DATA,
               user_input_files = USER_INPUT_FILES,
               user_output_files = USER_OUTPUT_FILES,
               n_procs = N_PROCS,
               n_procs_min = N_PROCS_MIN,
               n_procs_max = N_PROCS_MAX)

    # Possible SYRTHES coupling

    if os.path.isdir(os.path.join(casedir, 'DATA_SYR')):
        sd = syrthes_domain(syrthes_env = 'syrthes.env',  # SYRTHES paths file
                            echo_comm = None,             # coupling verbosity
                            coupling_mode = 'MPI',        # 'MPI' or 'sockets'
                            coupled_app_ids = 1)          # coupled app. ids
    else:
        sd = None

    # Now handle case for the corresponding calculation domain(s).

    case = case(casedir,
                d,
                sd,
                results_by_suffix = True,
                exec_preprocess = EXEC_PREPROCESS,
                exec_partition = EXEC_PARTITION,
                exec_solver = EXEC_SOLVER)

    # Select suffix (define if only running the solver or saving results)

    suffix = None

    # Force MPI environment if mpi_environment != None

    mpi_env = None

    # Syntax is as follows:
    #
    # mpi_env = mpi_environment()
    #
    # Some fields may need to be modified in case of incorrect defaults
    # (due to the wide variety of MPI implementations and build options,
    # the default configuration may not give correct values in some cases).

    # mpi_env.bindir = path to mpi binaries
    # mpi_env.mpiexec = mpiexec, mpirun, or equivalent command
    # mpi_env.mpiexec_args = option to pass arguments (usually None, or -args)
    # mpi_env.mpiexec_exe = option to define executable (usually None, of -exe)
    # mpi_env.mpiexec_n = option to define number of ranks (e.g. ' -n ', ' -np ')
    # mpi_env.gen_hostsfile = shell command to generate hostsfile if required
    # mpi_env.del_hostsfile = shell command to delete hostsfile if required
    # mpi_env.mpiboot = command to start environment (e.g. mpdboot, lamboot)
    # mpi_env.mpihalt = command to halt environment (e.g. mpdallexit, lamhalt)
    # mpi_env.mpmd = MPI_MPMD_mpiexec (mpiexec colon-separated syntax), or
    #                MPI_MPMD_configfile (mpiexec -configfile syntax), or
    #                MPI_MPMD_script, or
    #                MPI_MPMD_execve

    # Execute script

    case.run(n_procs=None,
             hosts_list=HOSTS_LIST,
             mpi_environment=mpi_env,
             exec_prefix=def_exec_prefix(),
             suffix=suffix,
             prepare_data=True,
             run_solver=True,
             save_results=True)

